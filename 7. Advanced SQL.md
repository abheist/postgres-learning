- Grouping
```sql
select * from employees;
select * from sales;
-- sales have an employee id as foreign key

select employee_id, sum(amount), array_agg(product)
from sales
group by employee_id;
-- just like, we have other functions like:
-- - avg
-- - min
-- - max
-- - any_value = picks one value from randomly
-- - sum

-- and array_agg will show all the values in array
-- same, json_agg will show in json format
-- bool_and: bool_and(amount > 100) as all_over_100
-- bool_or: bool_or(amount > 1000) as all_over_100

-- to filter after group by, you have to use 'having'
select
	employee_id
from
	sales
group by
	employee_id
having
	bool_or(is_returned) is false;


select 
	employee_id, 
	count(*),
	count(*) FILTER (where sales.is_returned is false) as non_returned,
	count(*) FILTER (where sales.is_returned is true) as returned
	string_agg(product, ', ') FILTER (where sales.is_returned is true)
		as retured_products
from sales
group by employee_id;

select 
	e.id,
	e.first_name || ' ' || e.last_name as employee_name,
	count(*),
	count(*) FILTER (where sales.is_returned is false) as non_returned,
	count(*) FILTER (where sales.is_returned is true) as returned
from sales
join employee e on sales.employee_id = e.id
group by e.id;
```
- Grouping sets, rollups, cubes
```sql
select
	employee_id, region, sum(amount)
from sales
group by
	grouping set (
		(employee_id), (region), ()
	);
-- here () blank group will give total

-- this will group in steps, first will do for all mentioned groups, then remove the last one and then continue in the same fashion, until it is empty.
select
	employee_id, region, sum(amount)
from sales
group by
	rollup (region, employee_id);

-- cube does all possible permutation and combinations
select
	employee_id, region, sum(amount)
from sales
group by
	cube (employee_id, region);
```
- Window functions
```sql
select * from sales;

select 
	*,
	avg(amount) over(partition by region),
	avg(amount) over() as avg
 from sales;

select
	*,
	first_value(id) over(partition by user_id order by id asc)
from bookmarks
limit 30;
```
- CTEs (Common Table Expressions)
```sql
-- This `with` expression is CTE
with new_cte as ()
select * from new_cte;
-- here we extracted the common part of the query and extracted, then it can be used at multiple places in long query.

with all_users as (
	select * from users
	union all
	select * from users_archive
)
select * from all_users where email = 'hi@abheist.com';

-- Sometimes postgres can't decide to Materialize or not to Materialize the CTE. If CTE is only get used once, it will not materialize it.

-- To control it, you can use
-- - not materialized
-- - materialized
with all_users as not materialized (
	select * from users
	union all
	select * from users_archive
);


with all_users as (
	select * from users
	union all
	select * from users_archive
)
abhis as (
	select * from all_users where email = 'hi@abheist.com';
)
select * from abhis;

-- This can provide much performance improvement if we are using the same query multiple times.
```
- Handling nulls
- Row value syntax
- Views
```sql
-- CTEs create commmon table on the fly and we are naming it. But with view, we can keep that combined table name for longer term. and that name can be used across the DB in dfferent call. But it will run the view query on the fly.

create view all_users as (
	select * from users
	union all
	select * from users_archive
);

-- once the view is created, now we can run queries on below anytime. and it will run the view query.
select * from all_users;

-- It is simply a names query. 
```
- Materialized views
```sql
-- Materialized view is same query View (names query), but the difference is here that data is stored in cache. 

create materialized view bookmarks_rollup_historic as (
	select saved_on,
		count(*)
	from bookmarks
	where saved_on < (CURRENT_DATE - INTERVAL '1' DAY)
	group by saved_on
);

select * from bookmarks_rollup_historic;
-- Here the result is coming from cached table
-- You can also create index on materialized view and actually that is good idea.

-- But this does get updated every now and then with every update. So either we can run single query every morning to update it.

-- But somewhere like dashboard where users needs fresh data combined from multiple tables, where actual query is slow. In this cases we can use normal view and materialized view together.

select * from bookmarks_rollup_historic
union all
select
		saved_on,
	count(*)
	from bookmarks
	where saved_on >= (CURRENT_DATE - INTERVAL '1' DAY)
	group by saved_on;
-- see here we flipped the gt sign. so most of the data will come from materialized view and rest will come from query, and then we unionizing them.

create view bookmarks_rollup as (
	select * from bookmarks_rollup_historic
	union all
	select
			saved_on,
		count(*)
		from bookmarks
		where saved_on >= (CURRENT_DATE - INTERVAL '1' DAY)
		group by saved_on;
);

-- now you can do, to get all the data, fresh and historic, 98% of the data will come from materialized view and append the latest data from query.
select * from bookmark_rollup;

-- at the end, we have to refresh materialized_view on regular basis
refresh materialized view bookmarks_rollup_historic;

-- to make it much better, add indexing on materialized view.
```
- Removing duplicate rows
```sql
-- to find the duplicates

with duplicates_identified as (
	select 
		*,
		row_number() over(partition bt user_id, url) > 1 as is_duplicate
	from bookmarks
	where user_id = 123123
),
duplicates as (
	select id from duplicates_identified where is_duplicates as true
)

select * from duplicates; -- these are all duplicate ids

-- to delete we can do something like this:
delete from bookmarks where id in (select id from duplicates);

-- or much optmized version, where we are not creating second CTE
delete from bookmarks where id in (
	select id from duplicates_identified where is_duplicate is true
);

```
- Upsert
```sql
-- this will do nothing if values already exists
insert into kv (key, value) values ('cache:foo', 123)
	on conflict (key) do nothing;

-- to update the value
insert into kv (key, value) values ('cache:foo', 123)
	on conflict (key) do update set value = excluded.value;

-- to update sometimes based on condition
insert into kv (key, value) values ('cache:foo', 123)
	on conflict (key) do update set value = excluded.value
	where kv.value is null;
-- here kv is referencing the old data
-- and excluded is referencing new data
```